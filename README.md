# just-triton-flash

This is a pip installable repo that has the name as `flash_attn` but doesn't make you install all the CUDA stuff if you just need one file.

[Source](https://github.com/HazyResearch/flash-attention)
